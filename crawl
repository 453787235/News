# --*--coding:utf-8 --*--
__author__ = 'Administrator'

import threadpool
import util
from lxml import etree
import json
import threading

redis = util.RedisUtils()
lock = threading.RLock()
flag = 0

class BaseCalss(object):

	def __init__(self):
		self.title = ''
		self.content = ''
		self.summary = ''
		self.from_source = ''
		self.display_section = ''
		self.operator_id = ''
		self.headers = {'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
						'Accept-Encoding':'gzip, deflate, sdch',
						'Accept-Language':'zh-CN,zh;q=0.8',
						'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',
						'Host':'insurance.hexun.com',
						}
		self.ns = util.News(self.headers)

	def crawl(self,url):
		lock.acquire()
		req = self.ns.Requests(url,self.headers)
		util.operatemysql(url)
		res = etree.HTML(req.content)
		res1 = req.text
		self.title = self.ns.gettitle(res1)
		self.content = self.ns.getcontent(res1)
		# self.summary = self.content[:150]
		# self.from_source = self.ns.getfrom_source(res,'')
		self.summary = ''
		self.from_source = ''
		self.display_section = 1
		self.operator_id = 0
		data = {'title':self.title,'content':self.content,'summary':self.summary,'from_source':self.from_source,'display_section':self.display_section,'operator_id':self.operator_id}
		print data
		result = self.ns.interface(data)
		lock.release()

if __name__ == '__main__':
	pool = threadpool.ThreadPool(10)
	bc = BaseCalss()
	while True:
		urls = redis.blpop('urls',100)
		if urls is not None:
			link = json.loads(urls[1]).get('urls')
			requests = threadpool.makeRequests(bc.crawl, link)
			[pool.putRequest(req) for req in requests]
			pool.wait()
		else:
			# 没有获取到任务
			print '没有获取到任务'
			flag += 1
			if flag >= 600:
				break
